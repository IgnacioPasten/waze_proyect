# ================================
# Dockerfile para hadoop-pig personalizado
# ================================

FROM ubuntu:22.04

ENV DEBIAN_FRONTEND=noninteractive

# 1. Instalar dependencias básicas (curl, ssh, OpenJDK 8, wget)
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
      openjdk-8-jdk-headless \
      ssh \
      rsync \
      wget \
      vim \
      ca-certificates \
      procps \
    && rm -rf /var/lib/apt/lists/*

# 2. Crear usuario hadoop (puedes ejecutar Hadoop sin root)
RUN useradd -m -s /bin/bash hadoop && \
    mkdir -p /home/hadoop/.ssh && \
    ssh-keygen -t rsa -P "" -f /home/hadoop/.ssh/id_rsa && \
    cat /home/hadoop/.ssh/id_rsa.pub >> /home/hadoop/.ssh/authorized_keys && \
    chown -R hadoop:hadoop /home/hadoop

RUN echo 'export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64' >> /home/hadoop/.bashrc

# 3. Variables de entorno de Java
ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
ENV PATH=$PATH:$JAVA_HOME/bin

# 4. Descargar y descomprimir Hadoop 3.3.1 (o otra versión 3.x estable)
ENV HADOOP_VERSION=3.3.1
ENV HADOOP_HOME=/opt/hadoop
ENV HADOOP_DOWNLOAD_URL=https://downloads.apache.org/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz

RUN wget -qO- $HADOOP_DOWNLOAD_URL \
    | tar -xz -C /opt/ && \
    mv /opt/hadoop-${HADOOP_VERSION} /opt/hadoop && \
    chown -R hadoop:hadoop /opt/hadoop

# 5. Configurar variables de entorno de Hadoop
ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
ENV HADOOP_COMMON_HOME=$HADOOP_HOME
ENV HADOOP_HDFS_HOME=$HADOOP_HOME
ENV HADOOP_MAPRED_HOME=$HADOOP_HOME
ENV HADOOP_YARN_HOME=$HADOOP_HOME
ENV YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
ENV PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

# 6. Instalar Pig (p.ej. 0.17.0)
ENV PIG_VERSION=0.17.0
ENV PIG_HOME=/opt/pig
ENV PIG_DOWNLOAD_URL=https://downloads.apache.org/pig/pig-${PIG_VERSION}/pig-${PIG_VERSION}.tar.gz

RUN wget -qO- $PIG_DOWNLOAD_URL \
    | tar -xz -C /opt/ && \
    mv /opt/pig-${PIG_VERSION} /opt/pig && \
    chown -R hadoop:hadoop /opt/pig

ENV PATH=$PATH:$PIG_HOME/bin

# 7. Copiar archivos de configuración Hadoop
#    Se asume que, en el contexto de “docker build”, existe la carpeta ./config con los 4 XML:
#       - core-site.xml
#       - hdfs-site.xml
#       - mapred-site.xml
#       - yarn-site.xml
COPY config/ $HADOOP_CONF_DIR/

# 8. Copiar init-hadoop.sh y darle permisos
COPY init-hadoop.sh /init-hadoop.sh
RUN chmod +x /init-hadoop.sh && chown hadoop:hadoop /init-hadoop.sh

# 9. Crear carpeta HDFS local (NameNode) para persistencia
RUN mkdir -p /hadoop/dfs/name && chown -R hadoop:hadoop /hadoop

# 10. Cambiar a usuario hadoop para ejecutar Hadoop
USER hadoop
WORKDIR /home/hadoop

# 11. Comando por defecto: al iniciar el contenedor, ejecutamos init-hadoop.sh
#     Dicho script debe formatear NameNode, arrancar dfs/yarn y luego hacer tail -f
CMD ["/init-hadoop.sh"]
