FROM ubuntu:22.04
ENV DEBIAN_FRONTEND=noninteractive

# 1) Instalar dependencias (Java 8, OpenSSH Server, curl, etc.)
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
      openjdk-8-jdk-headless \
      openssh-server \
      rsync \
      curl \
      vim \
      ca-certificates \
      procps \
    && rm -rf /var/lib/apt/lists/*

# 2) Crear usuario 'hadoop' y configurar SSH keys
RUN useradd -m -s /bin/bash hadoop && \
    mkdir -p /home/hadoop/.ssh && \
    ssh-keygen -t rsa -P "" -f /home/hadoop/.ssh/id_rsa && \
    cat /home/hadoop/.ssh/id_rsa.pub >> /home/hadoop/.ssh/authorized_keys && \
    chown -R hadoop:hadoop /home/hadoop

# 2.1) Configurar sshd para permitir login con clave
RUN mkdir -p /var/run/sshd && \
    sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config && \
    sed -i 's/#PasswordAuthentication yes/PasswordAuthentication no/' /etc/ssh/sshd_config

# 3) Variables de entorno
ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
ENV HADOOP_VERSION=3.3.5
ENV HADOOP_HOME=/opt/hadoop
ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
ENV PIG_HOME=/opt/pig
ENV PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PIG_HOME/bin

# 4) Instalar Hadoop
RUN curl -fSL "https://downloads.apache.org/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz" -o /tmp/hadoop.tar.gz \
 && tar -xzf /tmp/hadoop.tar.gz -C /opt \
 && mv /opt/hadoop-${HADOOP_VERSION} $HADOOP_HOME \
 && chown -R hadoop:hadoop $HADOOP_HOME \
 && rm /tmp/hadoop.tar.gz

# 5) Instalar Pig
RUN curl -fSL "https://downloads.apache.org/pig/pig-0.17.0/pig-0.17.0.tar.gz" -o /tmp/pig.tar.gz \
 && tar -xzf /tmp/pig.tar.gz -C /opt \
 && mv /opt/pig-0.17.0 $PIG_HOME \
 && chown -R hadoop:hadoop $PIG_HOME \
 && rm /tmp/pig.tar.gz

# 6) Configuración del entorno
RUN echo "export JAVA_HOME=$JAVA_HOME" >> /home/hadoop/.bashrc && \
    echo "export HADOOP_HOME=$HADOOP_HOME" >> /home/hadoop/.bashrc && \
    echo "export PIG_HOME=$PIG_HOME" >> /home/hadoop/.bashrc && \
    echo "export PATH=$PATH" >> /home/hadoop/.bashrc && \
    mkdir -p /opt/hadoop/logs && \
    chown -R hadoop:hadoop /opt/hadoop/logs

# 7) Copiar configuraciones
COPY config/ $HADOOP_CONF_DIR/
COPY config/pig-log4j.properties $PIG_HOME/conf/log4j.properties
RUN ln -s $HADOOP_CONF_DIR /opt/hadoop/conf

# 8) Configurar Pig para Hadoop 3
RUN echo "pig.use.overriden.hadoop.configs=true" >> $PIG_HOME/conf/pig.properties && \
    echo "fs.defaultFS=hdfs://hadoop:8020" >> $PIG_HOME/conf/pig.properties

# 9) Preparar directorios HDFS
RUN mkdir -p /hadoop/dfs/name /hadoop/dfs/data \
 && chown -R hadoop:hadoop /hadoop/dfs

# 10) Copiar script de inicialización
COPY init-hadoop.sh /init-hadoop.sh
RUN chmod +x /init-hadoop.sh && chown hadoop:hadoop /init-hadoop.sh

# 11) Configuración final
WORKDIR /home/hadoop

# Crear symlinks globales para hdfs/yarn
RUN ln -s $HADOOP_HOME/bin/hdfs /usr/bin/hdfs && \
    ln -s $HADOOP_HOME/bin/yarn /usr/bin/yarn

# Crear symlinks globales para hdfs/yarn y configuraciones
RUN ln -s $HADOOP_HOME/bin/hdfs /usr/bin/hdfs && \
    ln -s $HADOOP_HOME/bin/yarn /usr/bin/yarn && \
    echo 'export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native' >> /etc/profile && \
    echo 'export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"' >> /etc/profile && \
    ln -sf $HADOOP_HOME/libexec/hdfs-config.sh /usr/bin/hdfs-config.sh && \
    chmod +x $HADOOP_HOME/libexec/*.sh && \
    ln -sf $HADOOP_HOME/libexec/*-config.sh /usr/bin/ 2>/dev/null || true

CMD ["/init-hadoop.sh"]